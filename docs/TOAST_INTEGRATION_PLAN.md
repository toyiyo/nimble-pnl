Toast POS Orders Integration Implementation Plan
Scope of Data Ingestion
We will ingest full order data from Toast POS, capturing every component of an order. This includes all menu items (line items) with their prices and quantities, any discounts applied (both item-level and check-level discounts), all taxes calculated on the order, any tips or gratuities added through payments, and any refunds or voids processed on orders. The goal is to have a complete record of each order in our system, mirroring Toast’s data. The Toast webhooks provide a complete JSON representation of each order update, including line items, pricing, discounts, taxes, tips, and more[1][2]. By capturing this full detail, our integration can accurately compute sales metrics and handle adjustments like discounts and refunds in our internal models.
Architecture Overview
Our integration will use a combination of real-time webhooks and scheduled batch jobs to reliably ingest and sync order data from Toast into the multi-tenant Supabase system. The architecture has several key components:
    •    Real-Time Webhook Ingestion: We will subscribe to Toast’s orders webhook (event type order_updated) for each restaurant. Whenever an order is created or updated in Toast, Toast will send an HTTP POST to our Supabase Edge Function endpoint with the full order JSON[1]. This provides low-latency, realtime ingestion of new orders and updates. The webhook payload includes the restaurant’s GUID and a complete Order object with all details[3][4]. Our function will verify the webhook’s authenticity and then enqueue the order data for processing.
    •    Scheduled Reconciliation via /ordersBulk: In addition to webhooks, we will run a nightly batch job to pull orders via the Toast REST API’s /ordersBulk endpoint. This serves two purposes: (1) Initial historical sync – when a restaurant first connects, we can import the last ~90 days of orders in bulk, and (2) Recovery for missed updates – to reconcile any orders that might have been missed or if our service was down when a webhook was sent. Toast’s API allows fetching all orders modified in a given time range[5]. Toast officially recommends using webhooks for ongoing updates rather than constant polling[6], so our batch job will be run infrequently (once per day at off-peak hours) to minimize load. It will use startDate/endDate filters to retrieve orders modified in the last day (or the needed range) and page through results [100 orders per page max][7]. This ensures our data set is eventually consistent with Toast (catching any data that webhooks might have missed). We will also use the batch job for the one-time 90-day backfill when setting up a new restaurant.
    •    Anti-Corruption Layer (ACL) – Data Mapping: Between Toast’s API/webhook format and our internal database, we will implement an Anti-Corruption Layer[8]. This is a translation layer that maps Toast’s nested JSON structures to clean internal Data Transfer Objects (DTOs) that match our database schema. Toast’s data model (orders with nested checks, menu item selections, applied discounts, taxes, payments, etc.) will be converted by this layer into our own normalized format. The ACL ensures our internal system is not tightly coupled to Toast’s schema and semantics[9]. For example, Toast’s MenuItemSelection objects, which contain item details, quantities, discounts, and taxes, will be converted into a simple internal representation of a line-item sale and any associated discount/tax adjustments. This mapping layer will encapsulate all Toast-specific quirks (field names, nested arrays, etc.), producing clean internal order data objects that the rest of our system can work with.
    •    Access Token Caching & Refresh: Toast’s API uses OAuth 2.0 client credentials for authentication – we obtain a Bearer token by calling the Toast authentication endpoint with our client ID and secret[10]. These tokens expire after a limited time [Toast’s response includes an expiry timestamp][11]. To handle this, we will cache access tokens per restaurant in the database and refresh them proactively. A table (e.g. toast_tokens) will store each restaurant’s current token and its expiration time. Whenever an Edge Function needs to call Toast’s API (e.g. the bulk sync job), it will check if the token is nearing expiration. If the token is expired or will expire soon, the function will use the stored client credentials to request a new token from POST /authentication/v1/authentication/login[12][13], update the toast_tokens table, and then proceed. This way, each restaurant’s token is reused efficiently and refreshed in the background before it expires, avoiding failed API calls. Token refresh errors (e.g. invalid credentials) will be caught and alerted (see Error Handling below). By caching tokens, we eliminate unnecessary login calls while ensuring we always have a valid token when making Toast API requests.
    •    Data Normalization into Unified Sales Format: After using the ACL to parse Toast’s order data, we will normalize it into our internal schema, specifically a unified_sales table that integrates with our existing systems. Our goal is to represent each order’s financial components in a unified, relational format that’s consistent across different integrations. The unified_sales table will contain one row per financial component of an order, categorized by type (e.g. base revenue, tax, tip, discount, refund). By normalizing data this way, we can easily aggregate and query across all sales regardless of source. For each Toast order, we will break down the data as follows:
    •    Revenue – For each menu item sold (or each check), we record the net revenue (the final price charged to the customer for that item or check). Toast’s price field on a menu item selection is the final price after any item-specific discount, excluding tax[14][15]. We will sum up item prices (or use Toast’s check-level amount which is the total of item prices minus discounts[16]) to get the net sales amount. Each distinct sale item can be an entry. For example, if an order has a $16 pre-discount total and a $1 discount, the item(s) net revenue is $15[17][18].
    •    Discounts – Any discount given will be recorded as a separate entry. Item-level discounts are inherently reflected in reduced item prices, but we will still capture their amount for reporting. For instance, the $1 off in the above example is recorded as a discount entry of $1[17]. Check-level discounts (from Toast’s check.appliedDiscounts) will also be captured (e.g. a 10% off the entire check would be an entry). All discount amounts will be stored as positive values in unified_sales with type 'discount' (indicating a reduction in revenue). This allows calculating total discounts given and recomputing gross sales. [Gross revenue can be derived as net revenue plus discounts][14][15].
    •    Taxes – We will create entries for taxes collected. Toast calculates tax per item (and per service charge) and provides a total tax per check [taxAmount][19][20]. In our unified model, we likely record one aggregate tax entry per order (or per check) with the total tax amount. For example, if the $15 net sale had a state and city tax totaling $1.35, we record a tax entry of $1.35[21][22]. (If needed, we could break out multiple tax entries by tax type, but unless required, a single combined tax entry suffices for overall revenue calculations.)
    •    Tips – Any tip or gratuity paid via the POS will be recorded. In Toast, each payment in the order has a tipAmount field if a tip was added[23][24]. We will sum all tips across payments on the order and insert one or more tip entries. Typically, if an order is paid with one credit card including a tip, that yields a single tip entry. If multiple checks or payments have tips, we can record each, or a single combined tip for the order. Tips are stored with type 'tip' and the amount the customer tipped. These are not part of revenue (they are often passed through to staff), but we track them to know total collected vs. actual sales[25].
    •    Refunds – If an order (or payment) is refunded, we record a refund entry. Toast indicates refunds in the payment data: if a payment was refunded, refundStatus and refund.refundAmount will be set for that payment[26][27]. A refund reduces the business’s revenue, so in unified_sales we represent it as an adjustment. We will store the refunded amount as a positive number with type 'refund' (to indicate money paid back to the customer). If a refund covers the entire check (including tax), the refundAmount from Toast will typically equal the original charge amount (excluding tip), and tipRefundAmount might indicate if tip was also returned[26][27]. Our logic will capture both: a refund entry for the base/tax amount refunded, and if tips were also refunded, we could either reduce the tip entry or log a separate refund entry for the tip portion. (In many cases, tips might not be refunded, but we will handle both scenarios for completeness.) All refund entries allow us to offset revenue in calculations of net sales.
All these components will link back to the original order (via an order_guid or internal order ID) and the restaurant, so we maintain referential integrity. The normalized unified_sales schema is described more in the Database section, but essentially it enables easy summation of totals by category. By mapping Toast’s data into these categories, we ensure compatibility with existing reporting queries and cross-platform analytics.
Multi-Tenant Support
Our system is multi-tenant, meaning each restaurant’s data and credentials are isolated. We will leverage Supabase’s multi-tenant capabilities, including Row-Level Security, to ensure strong data isolation[28].
    •    Tenant Credentials & Configuration: Each restaurant (tenant) will provide their own Toast API credentials: a Client ID, Client Secret, and the restaurant’s GUID (Toast’s unique identifier for the location). We will store these in a secure configuration table (e.g. restaurants or toast_tokens table). The clientSecret is sensitive and will be encrypted at rest. For example, we can use Postgres pgcrypto to encrypt the secret using a symmetric key, or use an environment secret for encryption. The encrypted secret (or a hash) is stored, so the plaintext is never exposed directly in the DB. The table will also store the Toast restaurant_guid to identify the tenant in webhook payloads and API calls, and possibly a reference to the Supabase Auth user or organization that owns that restaurant.
    •    Row Level Security (RLS): All tables that store tenant-specific data will have RLS policies to enforce isolation[29]. Each row will have a restaurant_id (our internal tenant identifier, or could directly use the Toast GUID as an identifier) that links it to a tenant. RLS policies will ensure that users can only access rows where the restaurant_id matches their own. For example, for a table like unified_sales, we might create a policy: CREATE POLICY tenant_isolation ON unified_sales FOR SELECT USING (restaurant_id = auth.uid()) if auth.uid() represents the current user's restaurant. If a user account can have access to multiple restaurants, we would use a claim or join to a membership table instead. The principle is that each restaurant’s data is siloed, and even if another restaurant uses the system, they cannot read or write each other’s rows. This applies to integration_orders_log, toast_orders, unified_sales, etc. (The only exception is our internal service role, which bypasses RLS for admin tasks like the scheduled sync).
    •    Supabase Auth and Tenant Context: We will leverage Supabase Auth to scope function calls to the correct tenant. For instance, if an authenticated user calls a function (e.g. to trigger a manual sync), their JWT will include their user ID and possibly a custom claim for restaurant_id. Our Edge Functions can decode the JWT to determine which tenant is making the call. We will then ensure any data access/modification in that request is for that tenant only (e.g. only syncing their restaurant_guid). For the automated processes (webhook receiver and nightly sync), those run without a user context, so they will use the service role to perform cross-tenant operations. In those cases, we explicitly look up which tenant a piece of data belongs to and set the restaurant_id accordingly when inserting. For example, the webhook payload includes restaurantGuid in the JSON[30]; the webhook function will map that GUID to our internal restaurant_id (via the config table) and tag all inserted rows with that ID. This way, even though the function runs with full rights, the data is labeled so that subsequent queries by users will be restricted to their own restaurant.
    •    Credential Isolation and Secrets: Each restaurant’s API credentials are stored separately and never shared. Users will not have direct access to the plaintext credentials or tokens of any restaurant, even their own. The Supabase Edge Functions will use the service role (which bypasses RLS by design when needed) to read the encrypted client secret and tokens in order to communicate with Toast’s API. Regular users cannot query these tables to retrieve secrets. Additionally, any global secrets (for example, an encryption key for the client secrets, or the webhook signing secret if one is global) will be stored in Supabase’s protected environment secrets and accessed via Edge Function environment variables[31]. This ensures sensitive keys are not hard-coded or exposed.
    •    Tenant-Specific Webhook URLs: (Optional consideration) We could use a single webhook endpoint for all restaurants and identify the tenant by the restaurantGuid in the payload (which is our plan), or we could set up separate endpoints per tenant. The simpler approach is one endpoint – the payload tells us which restaurant it’s for[30], and we route accordingly. We maintain a mapping of Toast restaurantGuid to our internal tenant ID, established when the restaurant sets up the integration (they provide the GUID or we fetch it via Toast API). Using that map, the webhook function knows exactly which tenant’s data to update.
In summary, multi-tenancy is achieved via per-tenant credential storage, strict RLS policies for data isolation, and careful scoping in our functions to ensure each restaurant’s integration operates in its own silo. This guarantees that data from one restaurant cannot be accessed or interfered with by another, which is crucial for security in a multi-restaurant SaaS environment[29].
Supabase Edge Function Design
We will implement two main Supabase Edge Functions in TypeScript to handle Toast integration: one for webhook ingestion and one for bulk syncing. Both will run in Supabase’s Deno environment and utilize the Supabase JS client (with service role privileges) to interact with our database. Below we outline the design and responsibilities of each function:
1. Webhook Receiver Function (toastWebhook)
This Edge Function will be the public endpoint that Toast calls for every order_updated event. Its design considerations:
    •    Endpoint & Security: We will configure the webhook URL in the Toast developer portal to point to this function (e.g. https://<project>.functions.supabase.co/toastWebhook). We expect Toast to sign their webhook messages for verification. According to Toast’s documentation, the webhook requests include an HMAC signature (and possibly a timestamp) in the headers for security[32]. We will store the known signing secret (if Toast provides one) as an environment variable or in our restaurants table for each tenant. Upon receiving a request, the function will compute the HMAC of the payload (using the secret or perhaps the client secret as key, per Toast’s spec) and compare it to the signature header. If the signature does not validate, the function will immediately respond with 401 Unauthorized and log an alert – this ensures we only process authentic calls from Toast and drop any forged or malformed requests.
    •    Fast Acknowledgment: To prevent Toast from timing out or retrying unnecessarily, the function will aim to respond quickly (Toast’s webhook guidelines likely specify a timeout and retry policy). Our strategy is to perform minimal work synchronously, then send a 200 OK. Specifically, we will: verify the signature, parse the JSON body, and enqueue the data for processing. Enqueuing will involve writing the payload to our integration_orders_log table (this write is quick, just inserting the JSON and some metadata). By doing this first, we durably capture the webhook data. We can then safely respond 200 to Toast to acknowledge receipt, typically within a short time (far under the 30-second timeout limit).
    •    Enqueueing vs. Direct Processing: After logging the payload, we have options: (a) hand off further processing to a background job or (b) continue processing in the same function invocation. Since Supabase Edge Functions run to completion (and we want to ensure the data is processed in near real-time), we may proceed to transform and insert the data right after logging, if the operations are fast. A single order insert should be relatively quick (sub-second). However, to be safe, we could also implement a background task mechanism: for example, insert the raw payload with a processed=false flag, then signal a separate processing routine (perhaps via a PostgreSQL NOTIFY or a scheduled function that picks up unprocessed logs). Supabase doesn’t have a built-in job queue, but we could leverage DB triggers. For simplicity, we will likely process inline: our tests show an order JSON (even up to 600KB[32]) can be parsed and inserted well within a second or two. We will keep the response fast by not performing any external API calls in this path (no calls to Toast or other services – just internal computation and DB writes).
    •    Parsing & Mapping: The function will parse the JSON body to extract relevant fields. The webhook payload structure is like: { timestamp, eventCategory, eventType, guid (eventId), details: { restaurantGuid, order: { ...OrderObject... } } }[33][34]. We will retrieve the restaurantGuid from details.restaurantGuid and find the corresponding internal restaurant_id. Then we retrieve the order object, which contains the full order state. This is passed into our mapping logic (ACL) which will translate it to internal DTOs. For example, from the order we might create an InternalOrder object with high-level fields (order GUID, business date, etc.) and lists of InternalLineItem, InternalDiscount, etc. The mapping will handle nested arrays: Toast’s order.checks[*].selections[*] array for items, appliedDiscounts arrays, payments array, etc., converting them to flat lists of entries.
    •    Database Writes: After mapping, the function (still within the same invocation) will perform upsert/insert operations:
    •    Insert a record into toast_orders (the denormalized orders table) or update it if this order GUID already exists. We use the order’s GUID as a unique key. Each incoming webhook is essentially the latest state of the order (Toast sends the complete order on every change[1]). We will compare the modifiedDate of the incoming order with what we have stored. The toast_orders table stores the last modifiedDate we processed for that order. If the incoming modifiedDate is newer, we proceed to update; if it’s equal or older (which could happen if out-of-order events or a duplicate delivery), we will drop this payload as stale (no changes made) to avoid processing the same update twice. Deduplication by Toast order GUID and timestamp ensures idempotency.
    •    Insert into integration_orders_log was done earlier (with the raw JSON, event_guid, etc.). We may update that log entry’s processed status now (mark it true and possibly store processing time or any error encountered).
    •    Insert into unified_sales: Before inserting new entries, we will remove any existing entries for that order to avoid duplicates. (Alternatively, we could update in place, but since the structure can change – e.g., an item added – it’s simpler to delete old rows and insert fresh). We use the mapped DTOs: for each revenue item, discount, tax, tip, refund from the order, create a row in unified_sales with the appropriate type and amount. For example, an order where an item was added will now produce more revenue entries than before, but an order that had its quantity changed will simply result in new entries replacing old ones. We wrap the deletion and insertion in a transaction to maintain consistency.
    •    All inserted rows will have the restaurant_id set to ensure RLS policies allow the owning tenant to query them (and no one else).
    •    Response: After successful processing, return HTTP 200. If any error occurs during processing (e.g., database downtime), we have two strategies: (1) respond with a non-2xx error so that Toast will retry the webhook (Toast typically retries on failure with exponential backoff, which is helpful for resilience), or (2) respond 200 but log the failure for manual reprocessing. The safer approach is to return an error to Toast if we didn’t successfully record the data, so we will likely do that to let Toast retry. We will ensure any partial transaction is rolled back to avoid half-done inserts on a failure.
    •    Logging & Alerting: The function will log important events: signature verification failures (with no sensitive info, just that a failure occurred for a restaurant), which we can monitor (these might indicate a misconfigured secret or a malicious request). It will also log processing exceptions. We could maintain an integration_errors table to insert a row whenever something goes wrong (including timestamp, restaurant_id, error type/message). This can be monitored by an admin or trigger an email. For example, if verifying a Toast webhook signature fails repeatedly, we’d alert that the signing secret might be wrong and needs attention. Similarly, if a DB insert fails or our data mapping throws an exception, we log that so we can fix any data issues.
2. Bulk Sync Function (toastBulkSync)
This Edge Function will handle the scheduled nightly synchronization of orders via the Toast REST API. Its responsibilities:
    •    Scheduling: We will use Supabase’s scheduled function feature (Supabase Cron) to invoke this function once per night (e.g., 3:00 AM local time, or a time configurable via env). The schedule can be configured in the Supabase dashboard or config (Cron syntax). When triggered, the function will run with our service role privileges (no specific user). We could also provide a way to call it on-demand (secured by an admin token or require an authenticated admin user) for troubleshooting or initial sync.
    •    Iterating Tenants: The function will loop through each active restaurant integration in our system. We will query the restaurants or toast_tokens table for all restaurants that have Toast enabled and iterate over them. This design ensures one slow or failing restaurant does not stop the others – we will wrap each restaurant’s sync in a try/catch and continue to the next on error.
    •    Token Handling: For each restaurant, we fetch their current access token and expiration from toast_tokens. If the token is absent or expired, we perform the OAuth client credential flow to get a fresh token[11][13]. This is done by an HTTP POST (which we can perform using Deno’s fetch API or a Toast SDK if available) with the client ID/secret. On success, we update the toast_tokens table for that restaurant (new token and new expiry time) and proceed. If the token refresh fails (e.g., invalid credentials or network issue), we log an error for that restaurant, possibly notify admins, and skip syncing for that restaurant in this run (to avoid endless errors). The next restaurants will still be processed.
    •    Determining Sync Range: We need to decide what time window of orders to fetch. Strategy:
    •    For the initial sync (first time setup), we want ~90 days of history. We can detect this if we have a flag in the restaurants table like initial_sync_done = false, or if our toast_orders table is empty for that restaurant. In that case, we will perform a one-time extended sync: e.g. set startDate = (today - 90 days), endDate = now. (If 90 days is too large to fetch in one go, we could break it into 30-day chunks in one invocation, or simply rely on paging which Toast provides.)
    •    For regular nightly sync, an easy approach is to sync the last 24 hours (or since the last recorded sync time). We could track last_sync_time per restaurant in a table. For example, after each successful run, store last_sync_time = now() for that tenant. Then next run, use startDate = last_sync_time - a small buffer and endDate = now(). A buffer (e.g. 1 hour overlap) can cover any late modifications that might have just missed the last cutoff. Alternatively, simply syncing yesterday’s business date could be done using the businessDate parameter (Toast allows querying by business date[35]). However, using startDate/endDate with last modification times is likely more precise. We will likely use modifiedDate window, as Toast suggests, to capture both new and updated orders in that range[36].
    •    Example: If today is Jan 7, 3am, we set startDate = Jan 6 00:00:00 UTC, endDate = Jan 7 00:00:00 UTC (covering yesterday in full). This would retrieve all orders created or modified on Jan 6th. If our webhooks worked perfectly, this might yield nothing new (all changes already captured), but if any were missed, they’ll appear here. For initial sync, startDate = Oct 9 00:00:00 (90 days ago) and endDate = now.
    •    Fetching Orders via /ordersBulk: With the token and restaurant GUID, the function will call Toast’s Orders API. Each call will be a GET request to /orders/v2/ordersBulk?startDate=...&endDate=...&pageSize=100&page=N with the appropriate headers: Authorization: Bearer <token> and Toast-Restaurant-External-ID: <restaurant_guid>[37][38]. We will start with page=1 and iterate:
    •    Perform fetch, parse JSON response (which will be an array of Order objects).
    •    For each Order in the array, process it through the same mapping and database upsert logic as the webhook function. Essentially, we reuse the ACL mapping to get internal DTOs and upsert into toast_orders and insert into unified_sales. We apply the same deduplication: check modifiedDate against what’s in toast_orders to avoid processing an order update that we’ve already seen via webhook. (Since we update toast_orders in real-time, it will have the latest mod date for each order we've seen. Any order from bulk with an older or equal mod date can be skipped safely.) If an order is new (e.g., created when our service was down) or has a later modification, we will upsert it and refresh unified_sales accordingly.
    •    Handle paging: if the response contains 100 orders (the max page size), there might be more pages. We increment page and repeat until we get less than 100 or no results. (Toast also provides pagination links in response headers[39], which we can use if easier.)
    •    We will be mindful of Toast API rate limits: /ordersBulk allows up to 5 calls per second per restaurant[40]. Our loop will likely do only a few calls per restaurant (e.g., one day of data might be maybe a few hundred orders, which is a few pages). We can easily throttle to <= 5 QPS if needed (e.g., add a small delay between page requests or simply not call in tight loops).
    •    Post-Sync Updates: After processing all pages for a restaurant, we can update that restaurant’s last_sync_time (if we track it). We will also mark if initial sync is completed (set initial_sync_done = true).
    •    Error Handling: If the API call fails (network error or HTTP >=400), we catch it. For token-related failures:
    •    If we get a 401 Unauthorized calling /ordersBulk, it could mean our token expired unexpectedly or scopes issue. We will attempt one token refresh on the spot (maybe the token just expired between our check and call). Then retry the call. If it still fails, we log an error for that restaurant and skip further processing for it. The error might indicate invalid credentials (if client revoked) – we would alert that the integration needs reauthorization.
    •    For other HTTP errors (>=500 from Toast or rate limit 429), we log the error. For a 429 rate limit, we could implement a short exponential backoff retry within the same run, but given our low frequency, hitting 5/sec is unlikely. If it does occur, we might pause for a second and retry.
    •    Any orders that were fetched before an error will have been processed; if we fail mid-page or mid-restaurant, we may not mark last_sync_time to avoid falsely thinking we synced fully. On the next run, those orders will be re-fetched, but our dedupe logic will skip those we already processed (idempotency ensured by order GUID and modDate).
    •    If one restaurant fails, the loop continues with the next restaurant so that others still sync.
    •    Closing and Metrics: After iterating all tenants, the function can log a summary (e.g., “X restaurants synced, Y orders processed, N errors”). If needed, we can store a small report in a table or send ourselves a notification on completion with any errors encountered. This helps operators monitor nightly jobs.
    •    Security: The bulk sync function should not be publicly accessible to anyone but our scheduler. We will deploy it such that it’s either not exposed via an open URL or it requires a special header/secret if someone tries to call it. For instance, we can check req.headers.get('Authorization') for a specific admin token or ensure that when the scheduler calls it, it does so internally. Supabase’s function scheduler likely calls it internally without requiring external HTTP access. In any case, we will double-protect it (so a malicious actor cannot trigger arbitrary sync runs or attempt to brute force data). The function uses the service role for DB access (so it can bypass RLS to read all tenants’ config and write their data), but it itself will enforce its own logic to only operate on each tenant’s data appropriately.
Database Schema Design
We will create the following tables (or use existing ones augmented) to support this integration. All tables include a restaurant_id (foreign key to a restaurants table identifying the tenant) to support multi-tenancy with RLS.
1. restaurants (Tenants Table) – if not already present.- Columns: id (PK), name, toast_restaurant_guid, toast_client_id, toast_client_secret_enc, toast_webhook_secret_enc, initial_sync_done (boolean), owner_user_id, etc.- Purpose: Stores each tenant restaurant’s identifying info and credentials. We will keep the Toast API credentials here (encrypted client secret) unless we choose a separate table. The toast_restaurant_guid helps map incoming webhooks to this record. The webhook signing secret (if Toast uses a separate secret for HMAC) can be stored here per restaurant (encrypted). We also include a flag for initial sync completion and possibly the user account that set it up (for auditing).- Security: RLS can be configured so that a restaurant owner can only see their own record (and even then, possibly only non-sensitive fields). The sensitive fields like secrets would either be omitted from any SELECT or returned encrypted. In practice, we might restrict toast_client_secret_enc and similar to no access except for the service role.
2. toast_tokens – Token Cache Table.- Columns: restaurant_id (PK/FK to restaurants), access_token, expires_at (timestamp), fetched_at (timestamp when we got this token).- Purpose: Caches the OAuth access token for each restaurant’s API access. When our system retrieves a new token, we insert or update this row. We could merge this into the restaurants table (fields for current token and expiry), but separating concerns can be cleaner. The table will typically have one row per restaurant (because each has its own token).- Usage: The Edge Functions will read this table to get the token for API calls. They will update it on refresh.- Security: We will apply RLS or simply restrict that normal users have no direct access to this table (since it’s not needed on the client side). The service role (Edge Functions) will use it. Storing the token itself in plain is usually okay (it’s short-lived and scoped), but if desired, we could encrypt it as well. Most likely not necessary due to short lifespan.
3. integration_orders_log – Raw Webhook Payload Log.- Columns: id (PK, bigserial), restaurant_id, event_guid (UUID of the Toast webhook event), order_guid (UUID of the order), event_time (timestamp from Toast’s timestamp field), payload (JSONB containing the full webhook body or order details JSON), processed_at (timestamp when we processed it, null if not yet processed), status (text, e.g. 'received', 'processed', 'error').- Purpose: Audit log of all incoming webhook events (and potentially entries from bulk sync if we choose to log those similarly). This table stores the exact payload we got from Toast for debugging or reprocessing. For example, if something goes wrong in processing, we can later replay the payload from here. It can also help detect duplicate events. We record the Toast event_guid to identify unique webhook events (Toast sends a unique ID for each webhook message[41]). The combination of event_guid or (restaurant_id, event_guid) can be unique-indexed to prevent accidental double-insert of the same event. We also store the order_guid for quick searching by order.- Growth: This table could grow large, but we can consider pruning older entries or moving them to archive after some time, since it’s mainly for audit.- RLS: Each restaurant’s entries are marked with their restaurant_id, so they can only (if we allow) query their own log. We might not even expose this to end users, but it’s protected regardless.
4. toast_orders – Denormalized Toast Order State.- Columns (key fields): restaurant_id (PK part), order_guid (PK part, Toast Order GUID), modified_date (timestamp of last modification we have), opened_date (timestamp order was opened), closed_date (timestamp if order was closed/paid), business_date (date or int as given by Toast, e.g. 20240107 format), order_json (JSONB of the latest order data), and possibly summary fields like total_amount (numeric, total check amount including tax), net_amount (numeric, net sales amount excluding tax/tip), tax_amount, tip_amount, status (text: e.g. OPEN, CLOSED, VOIDED).- Purpose: This table maintains a one-row-per-order snapshot of the latest state of each Toast order. It effectively de-normalizes the JSON into a storable form. We keep the raw JSON for reference, but also extract key fields that are useful for queries or comparisons (modified date, status, totals). This helps with deduplication logic (we check modified_date here) and can be useful if we need to join orders with other data (like linking an order to other systems). It’s also handy for debugging a specific order’s details without digging through logs.- Usage: On each webhook or bulk sync upsert, if order_guid is new, we INSERT; if it exists, we UPDATE the JSON and relevant fields if the incoming modified_date is more recent. If an order is marked as deleted or voided in Toast, we could update a status flag here.- RLS: Enforced by restaurant_id as usual. The data here, while detailed, doesn’t expose personal customer info (Toast omits PII in webhooks[42]), so it’s mostly order metadata. End users (restaurant owners) could potentially view their order records here if we build such UI, but primary analysis will use unified_sales.- Note: toast_orders essentially acts as a “Toast orders view”. We might not strictly need to query it often for analytics (since financials are in unified_sales), but it is useful for completeness and for linking any non-financial order info (order source, dining option, etc.) if needed later.
5. unified_sales – Unified Sales Ledger Table.- Columns: id (PK, maybe UUID or serial), restaurant_id, order_guid, entry_type (text or enum: 'revenue', 'discount', 'tax', 'tip', 'refund'), amount (numeric(12,2) for currency), business_date (date of the sale in restaurant’s local business calendar), occurred_at (timestamp of when this entry occurred, e.g. payment time or refund time), item_name (nullable text if we want to tag revenue with item identifier, not required for aggregate financials), check_guid (nullable, if we want to differentiate checks in an order), and timestamps inserted_at, updated_at.- Purpose: This is the core table that holds all financial components of sales in a standardized format. Every row is a monetary amount of a certain category, tied to an order. We will populate this exclusively via our integration logic (no direct user input). For Toast orders: - We will insert one or more 'revenue' rows for the order’s net item sales (we could aggregate all items into one revenue entry per order, or split by item for granularity – for financial reporting, the total matters, so one entry summing all items’ net revenue is enough. However, splitting by item might allow item-level sales analysis; but since that’s beyond the requested scope, we might aggregate at check/order level for simplicity). - 'discount' rows for each distinct discount applied (or one aggregated discount entry per order summing all discounts – again, depending on needed granularity of analysis. To know total discounts given, one aggregated entry per order is sufficient since we can sum them). We will likely log separate discount entries if needed to differentiate discount types, but otherwise can aggregate. - 'tax' row for total tax on the order (or per tax type if needed, but likely total). - 'tip' row(s) for tip amounts. - 'refund' row if applicable (covering the refunded amount).
Each entry has the order_guid so we can group entries belonging to the same order if needed. The business_date is important for reporting by date – we will use Toast’s concept of business date (which accounts for restaurant’s end-of-day cutoff) as provided in the order data[43][44]. This ensures that if a restaurant’s day goes past midnight, sales are correctly attributed. For refunds or tips that occur when the payment is processed, we might use the order’s business date as well, or we might set occurred_at to the actual timestamp of that transaction (especially if a refund occurs on a later date than the original sale, we could reflect that by business_date or at least by occurred_at).
    •    Example: Using our earlier example scenario (two $8 items with $1 discount, $1.35 tax, $2 tip, and later a refund of $16.35), the entries would be:• (restaurant_id = R1, order_guid = O123)– entry_type='revenue', amount=15.00 – net revenue from items [after $1 discount][18]– entry_type='discount', amount=1.00 – discount given [reducing $16 to $15][17]– entry_type='tax', amount=1.35 – tax collected on the $15 net [e.g. 5% + 4% tax][45][22]– entry_type='tip', amount=2.00 – tip collected from customer– entry_type='refund', amount=16.35 – refund issued (covering the $15 sale + $1.35 tax; tip was not refunded in this scenario)
These entries make it easy to compute metrics. (If the tip had also been refunded, we might instead see a refund of $18.35 and possibly an entry adjusting tip, but we’ll keep the scenario straightforward.)
    •    RLS: As with others, restaurant_id isolates tenants. Users will typically query aggregated results via dashboard charts or similar which internally use RLS to filter to their restaurant_id.
    •    Indexes: We will index restaurant_id, business_date, entry_type on unified_sales, as these are common query filters (e.g., sum revenue for a date range). We might also index order_guid if we need to quickly remove entries by order on updates (to support the upsert logic). Since updates will often delete old entries by order GUID, an index on order_guid is prudent.
Note on Schemas: The above tables can reside in the public schema, but with RLS. Alternatively, some teams use a dedicated schema for integration data. We will likely keep it simple with public schema, properly secured by RLS.
Relationships: restaurant_id in all these tables references the primary key of restaurants table. We will enforce foreign key constraints. This means if a restaurant is deleted or integration disconnected, we may need to cascade or archive their data accordingly.
Error Handling and Alerting
Robust error handling is vital for a reliable integration. We’ve built in several layers of deduplication and checks:
    •    Duplicate Order Deduplication: We use the Toast Order GUID as the unique key to avoid duplicate processing. In toast_orders, order_guid (per restaurant) will be unique. The upsert logic ensures we don’t insert a new row for the same order again. Moreover, we compare modifiedDate timestamps. Toast increments modifiedDate whenever the order changes[46], so we can identify stale data. If a webhook delivers an order state we already have or older, we simply ignore it (or mark it as skipped). This prevents double-counting. For example, if we received an order via webhook and then the bulk job finds it with the same modifiedDate, our code will see toast_orders.modified_date == incoming and skip re-inserting the unified_sales entries. Similarly, if two webhooks come in rapid succession, only the later one (with later mod time) will actually trigger an update; the first might insert, the second will update, but if a third duplicate of the second arrives (say due to retry), it will be identified as duplicate and dropped.
    •    Out-of-Order Events: In rare cases, network delays could mean a later update arrives before an earlier one. The modifiedDate check handles this naturally – the earlier event will have an older timestamp, so if we processed the newer state first, the older one will be discarded as stale. We thus always end up with the latest order state applied. (This assumes Toast’s modifiedDate is monotonically increasing per order, which is implied by their API.)
    •    Missing Webhooks: If a webhook was missed (e.g., our service was temporarily down or there was a network issue), the nightly bulk sync will catch the order. The bulk data gives the final state of the order. If we had partially processed it via an earlier webhook, the final state may have a later modifiedDate and will update accordingly; if we never saw it at all, it will be inserted anew. This ensures eventual consistency. We might also implement alerting for missed webhooks: e.g., if our logs show the service was down or if we notice via Toast’s dashboard that a webhook failed to deliver (Toast might have metrics), we could run the bulk sync on-demand.
    •    Error Logging: We will maintain an error log (either a separate integration_errors table or reuse integration_orders_log by marking status). For example, if a webhook comes with a bad signature, we add an entry noting “Signature verification failed for event X at time Y (possible invalid secret)”. If a token refresh fails, we log “Token refresh failed for Restaurant R (401 Unauthorized) at time Y – check client credentials”. These logs will help in debugging and also can be used to proactively notify someone.
    •    Alert Notifications: Critical failures – like inability to authenticate (which means we cannot fetch data) or repeated webhook signature mismatches – should trigger alerts. We can implement a simple notification system: e.g., sending an email to our support or engineering address via an SMTP integration or using a service like Slack webhook. Supabase Edge Functions can call external APIs, so we could invoke a Slack webhook or SendGrid email if certain errors occur frequently. Initially, we might monitor the error table manually.
    •    Retries: The integration leverages retries in two places: Toast’s side for webhooks (Toast will retry on non-2xx responses, so by returning 500 on processing failure we let Toast try again automatically) and our side for API calls (we will retry token fetch or bulk calls on certain failures as described). These retries reduce the chance of data loss due to transient issues.
    •    Transactions and Idempotency: Database operations that need to be atomic (like deleting old unified_sales entries and inserting new ones) will be wrapped in transactions. In case of a failure mid-transaction, it will roll back, preserving the previous state. This prevents partial updates. Our design also allows idempotent reprocessing: for example, if a webhook fails after we've inserted the raw log but before completing unified_sales insertion, on retry we might insert the log again (or we can check by event GUID to avoid duplicate log). Even if the log duplicates, the unified_sales upsert logic will still produce the correct final state (it might delete and reinsert the same entries again, which is harmless).
    •    Monitoring Data Consistency: We can periodically run a reconciliation check: e.g., ensure the sum of unified_sales revenue entries matches what Toast’s reports show for a given day. This could be a manual process or an automated check using the API (like call Toast’s summary endpoints if any). If we detect discrepancies, that might indicate missed data, and we could trigger a re-sync of that period.
In summary, the system is designed to fail safe: dropping duplicate or stale data, catching errors and either retrying or alerting as appropriate. With logging at each step and use of Toast’s reliable identifiers (GUIDs and timestamps), we maintain data integrity. Any detected issue will be surfaced either by automated retry or by an alert so that we can intervene quickly.
Security Considerations
Beyond multi-tenancy and error handling, there are specific security best practices we implement in this integration:
    •    Row Level Security (RLS): As noted, RLS policies are in effect on all tables to prevent unauthorized access[28]. Even though our Edge Functions use the service role (which can bypass RLS), any direct client access (e.g., if we expose some views in our app) will be mediated by RLS. This protects data at the lowest level. We will test RLS thoroughly to ensure no leaks (for example, we must be careful if using Supabase storage or any other avenues outside direct DB queries).
    •    Least Privilege for Functions: Our Edge Functions will use the service role for necessary operations (inserting data across tenants). However, where possible, we limit exposure: the webhook endpoint will be open (it has to be public for Toast), but it’s protected by the signature check so only Toast (knowing the secret) can use it. The bulk sync function will not be publicly accessible; if we do allow manual triggering, it will require an admin key or be invoked internally. Within the functions, we handle only the needed data for each tenant in scope – even though service role can see everything, we isolate per iteration.
    •    Secure Storage of Credentials: Client secrets and any sensitive keys are never stored in plaintext in the code or database. We utilize Supabase Secrets (Vault) for environment-level secrets like encryption keys or API keys[31]. For instance, if we choose to encrypt the Toast client secrets with a symmetric key, that key will live in Supabase’s secret store (accessible as an env var in the function, not in the DB). The client secrets themselves are encrypted in the DB using that key. This way, even if the database were compromised, the attacker cannot easily decrypt the secrets without the key (which is in a different system). Similarly, if Toast provides a webhook signing secret, we’ll store it in the DB encrypted or in env, and fetch it in the function when verifying.
    •    Encryption in Transit: All communication is over HTTPS: webhooks from Toast are via HTTPS endpoints, and our calls to Toast’s API are via HTTPS as well. Supabase handles HTTPS for the functions endpoints. We will also ensure to validate TLS certificates when the Edge Function calls Toast (this is default with fetch requests).
    •    No Sensitive Data Exposure: The data we store from Toast orders excludes personal guest info by design (Toast omits guest PII in webhooks[42]). We will double-check that – for example, if an order has a customer name or phone, Toast does not send it in the webhook (they explicitly list firstName, lastName, phone, etc. as omitted[47]). So we are not storing customer PII, mitigating privacy concerns. We are mainly handling financial transaction data. Still, we treat the data as sensitive business info: RLS and secure access are our protections.
    •    Function Resource Limits: Large payloads (up to 600KB) are expected[32]. We ensure our function can handle that by not doing extremely heavy computation on it. Deno can parse 600KB JSON easily; we just need to be mindful of memory if multiple calls come concurrently. Supabase likely scales function instances as needed. We also ensure to parse JSON safely (using JSON.parse on the body which is fine given size). If needed, we could stream the body, but not necessary for JSON of that size.
    •    Secret Rotation: If a client secret is compromised or needs rotation, Toast allows generating a new one[48][49]. Our design can handle that: the user could update their credentials in our system (we’d provide a secure UI to input the new secret which we encrypt and store). The next token refresh would use the new credentials. Similarly, if the webhook secret is changed (Toast might allow updating it), we update our stored secret accordingly to keep signature verification correct.
    •    Audit: With logs of events and possibly logging who triggered manual syncs, we maintain an audit trail. If any suspicious activity occurred (e.g., an unexpected spike in webhook failures or an unauthorized attempt to call the bulk function), we can investigate via the logs.
By following these practices, we ensure the integration not only functions correctly but also upholds the security and privacy of each restaurant’s data. Supabase’s platform features (RLS, secrets, etc.) and Toast’s secure API design (OAuth2, signed webhooks) are leveraged to build a secure end-to-end solution.
Mapping Example from Toast Order to Internal DTOs
To illustrate the data mapping and transformation, consider a concrete example order in Toast and how it becomes entries in our system:
Scenario: A restaurant has an order where a customer ordered 2 items, each $8.00 (so $16.00 total). A promotional discount of $1.00 was applied to one of the items. Sales tax is 5% state and 4% city (9% total) on the post-discount amount. The customer paid and added a $2.00 tip. Later, the customer had an issue and the restaurant refunded the entire order (the $15.00 net sale + $1.35 tax, but not the tip).
In Toast’s data model, when the order was finalized: - Each item is represented as a MenuItemSelection in the order’s JSON. For 2 items at $8 each, Toast would show preDiscountPrice = 16.0 for that selection [2×$8][50]. After the $1 discount, price = 15.0 [final price charged for the items][18]. The discount is likely listed under appliedDiscounts with discountAmount = 1.0[51]. - Taxes: Toast would calculate the tax on the $15.00: state tax 5% of 15 = $0.75, city tax 4% of 15 = $0.60. These might appear as two entries in appliedTaxes with taxAmount values 0.75 and 0.60[52]. The selection’s total tax field would be the sum $1.35[53]. On the check, taxAmount would also be $1.35[54]. - The tip $2.00 would be recorded in the Payment object [tipAmount = 2.00 on the payment][23]. - The refund, when processed, would update the Payment’s refundStatus to FULL and refund.refundAmount = 16.35 (the base + tax) and possibly refund.tipRefundAmount = 0 if the tip was not refunded[26][27]. The order’s paymentStatus might remain CLOSED but the payment entry shows refunded amounts.
ACL Mapping: Our anti-corruption layer will take this Toast order JSON and map it to internal structures: - First, an internal Order DTO is created with high-level info: order GUID, business date, timestamps, etc. We capture that this order’s total was $16.35 with a $1 discount, leaving $15 net + $1.35 tax. - Then, we produce a list of UnifiedSaleEntry DTOs for this order: - A revenue entry for $15.00 with type 'revenue'. (In our logic, we obtain this either by summing the price of each selection or directly from Toast’s check amount field which is $15.00 in this case[55]. The check’s amount equals the sum of item prices minus any discounts, excluding tax[16][56].) - A discount entry for $1.00 with type 'discount'. (We get this from summing item-level and check-level discounts. Here, Toast had no check-level discount, only an item-level $1.00, so we capture that. It matches Discount.discountAmount in the item’s appliedDiscount[51].) - A tax entry for $1.35 with type 'tax'. (From the check’s taxAmount[54] or summing applied taxes.) - A tip entry for $2.00 with type 'tip'. (From Payment tipAmount.) - A refund entry for $16.35 with type 'refund'. (From Payment refundAmount, indicating the portion of the charge returned to the customer.)
These DTOs are then inserted into the unified_sales table. The final contents for this order (order_guid = say abc123) would look like:
restaurant_id
order_guid
entry_type
amount
business_date
...
R1
abc123
revenue
15.00
2024-01-07
...
R1
abc123
discount
1.00
2024-01-07
...
R1
abc123
tax
1.35
2024-01-07
...
R1
abc123
tip
2.00
2024-01-07
...
R1
abc123
refund
16.35
2024-01-08
...
(Note: The refund occurred on the next day in this scenario, Jan 8, so we might set its business_date to Jan 7 still (the original order date) or treat it as Jan 8. This depends on reporting needs – often, refunds are reported on the day they occur. We can use the occurred_at to record actual date/time. For simplicity here, assume same business date.)
From this unified data, we can derive business insights. For example, Gross revenue for that order would be $16.00 (which is revenue + discount), Net revenue (after discount) is $15.00, and after the refund, the net revenue effectively goes to $0 for that order (since the $15 was negated by a $16.35 refund, leaving -$1.35 net if we considered tax impact – but since tax isn’t counted in revenue, the business lost $0 of revenue, and $1.35 of tax was returned to the customer/government). The tip $2.00 remains and is not part of revenue but is part of total collected and then paid out to staff.
This example shows how the ACL isolates Toast’s complexities: we didn’t expose the concept of “selections” or “checks” to our unified_sales; we simply translated it into meaningful financial entries in our terms. If Toast had more complex scenarios (like multiple checks per order), our mapping would handle each check’s portion similarly (we’d get multiple revenue entries or still aggregate them). If there were service charges or loyalty rewards, we could map those too (e.g., a service charge could be another entry type if needed, or lump into revenue if considered part of sales). The ACL ensures any such detail is converted appropriately.
Sample SQL Queries for Reporting
Once the data is in unified_sales, we can easily compute various sales metrics. Here are example SQL queries for gross revenue, net revenue, and a breakdown of collected amounts at POS, using the unified data:
    •    Gross Revenue (total sales before discounts, excluding taxes and tips): This is essentially the sum of net revenue plus any discounts given. Since our revenue entries are already net of discounts, we add back the discount entries to get gross. For example, to get the gross revenue for a given day:
SELECT   (SUM(CASE WHEN entry_type = 'revenue' THEN amount ELSE 0 END)   + SUM(CASE WHEN entry_type = 'discount' THEN amount ELSE 0 END)  ) AS gross_revenueFROM unified_salesWHERE restaurant_id = $1  AND business_date = '2024-01-07';
This query sums all revenue and discount amounts for the restaurant on January 7, 2024. The result gross_revenue reflects what sales would have been without any discounts (i.e. $16.00 in our example, matching the pre-discount total). Note: We exclude taxes, tips, refunds in this gross calculation by design[25] (gross sales typically refer to menu item sales before discounts and without extras).
    •    Net Revenue (total sales after discounts, and after refunds if any, excluding taxes/tips): Since our revenue entries already exclude discounts, the sum of revenue entries gives us net sales after discounts. To also account for refunds (money given back), we subtract those. For instance, net revenue for that day after refunds:
SELECT   (SUM(CASE WHEN entry_type = 'revenue' THEN amount ELSE 0 END)   - SUM(CASE WHEN entry_type = 'refund' THEN amount ELSE 0 END)  ) AS net_revenueFROM unified_salesWHERE restaurant_id = $1  AND business_date = '2024-01-07';
If no refunds occurred that day, this is just sum of revenue. If some orders were refunded, those refunds (recorded as positive amounts in refund entries) are subtracted to yield the true net revenue retained. In our example, the query for Jan 7 would yield $15.00 - $16.35 = -$1.35 if the refund happened the same day. However, assuming the refund was on Jan 8, then Jan 7 net_revenue would be $15.00 (since the refund isn’t counted on that date). Jan 8 would show a negative net if just the refund occurred without sales. Businesses might report refunds separately, but this demonstrates the flexibility. Essentially, we treat refunds as negative sales in the net revenue computation.
    •    Collected-at-POS Breakdown: To understand the flow of cash through the POS, we can sum up each component: sales, tax, tip, and refunds. For example, a monthly breakdown could be:
SELECT   DATE_TRUNC('month', business_date) AS month,  SUM(CASE WHEN entry_type = 'revenue' THEN amount ELSE 0 END) AS total_sales,  SUM(CASE WHEN entry_type = 'tax' THEN amount ELSE 0 END) AS total_tax_collected,  SUM(CASE WHEN entry_type = 'tip' THEN amount ELSE 0 END) AS total_tips_collected,  SUM(CASE WHEN entry_type = 'discount' THEN amount ELSE 0 END) AS total_discounts_given,  SUM(CASE WHEN entry_type = 'refund' THEN amount ELSE 0 END) AS total_refunds_issued,  /* Total collected = sales + tax + tip (inflows) minus refunds (outflow) */  (SUM(CASE          WHEN entry_type IN ('revenue','tax','tip') THEN amount          ELSE 0 END)   - SUM(CASE          WHEN entry_type = 'refund' THEN amount          ELSE 0 END)  ) AS net_collectedFROM unified_salesWHERE restaurant_id = $1  AND business_date >= '2024-01-01' AND business_date < '2024-02-01'GROUP BY DATE_TRUNC('month', business_date);
This query groups by month (here January 2024) and calculates: - total_sales (the sum of revenue entries, after discounts), - total_tax_collected, - total_tips_collected, - total_discounts_given, - total_refunds_issued, - and finally net_collected which represents the actual cash the restaurant handled for that period.
For our example, if January 2024 had just that one order and its refund, the result would be: - total_sales = $15.00- total_tax_collected = $1.35- total_tips_collected = $2.00- total_discounts_given = $1.00- total_refunds_issued = $16.35- net_collected = ($15+$1.35+$2) - $16.35 = $2.00.
Net collected ended up as $2.00, which makes sense: the restaurant only kept the $2 tip in the end (since the sale was refunded, the $15 sale and $1.35 tax were returned). The breakdown query makes it easy to see each component’s contribution.
These queries demonstrate how flexible and powerful the unified_sales table is for reporting. We can easily adjust filters (by date range, by restaurant, by entry_type) and do computations. The design aligns with typical financial reporting: Gross sales = sum of sales before discounts[14][15], Net sales = after discounts (and optionally after refunds), Taxes and Tips are tracked separately (since they are liabilities or pass-throughs, not part of net sales[25]), and we can compute actual cash collected which includes all money taken in minus money paid out.
All of this is achieved with relatively simple SQL because of the upfront work to normalize data. The plan above ensures that integrating Toast POS orders into our multi-tenant Supabase system is done in a secure, scalable, and analyzable way, using Supabase Edge Functions for ingestion and TypeScript logic to map complex POS data into our unified format, ready for actionable insights.
Sources:
    •    Toast Developer Guide – Orders Webhook [order_updated event structure][1][4]
    •    Toast Developer Guide – Orders API [bulk retrieval and recommendations][6][5]
    •    Toast Developer Guide – Authentication [OAuth client credentials flow][10][13]
    •    Toast Developer Guide – Order Object [price, tax, discount definitions][14][16]
    •    Supabase Documentation – Row Level Security for multi-tenant data isolation[28]

[1] [2] [3] [4] [30] [32] [33] [34] [41] [42] [43] [44] [47] Orders webhook - - Developer guide
<https://doc.toasttab.com/doc/devguide/devOrdersWebhookRef.html>
[5] [6] [7] [35] [36] [37] [38] [39] [40] Getting detailed information about multiple orders - - Developer guide
<https://doc.toasttab.com/doc/devguide/apiOrdersGetDetailedInfoAboutMultipleOrders.html>
[8] [9] Anti-corruption Layer pattern - Azure Architecture Center | Microsoft Learn
<https://learn.microsoft.com/en-us/azure/architecture/patterns/anti-corruption-layer>
[10] [11] [12] [13] [48] [49] Authentication and restaurant access - - Developer guide
<https://doc.toasttab.com/doc/devguide/authentication.html>
[14] [15] [16] [17] [18] [19] [20] [21] [22] [45] [50] [51] [52] [53] [54] [55] [56] Order object summary - - Developer guide
<https://doc.toasttab.com/doc/devguide/apiOrdersOrderObjectSummary.html>
[23] [24] [26] [27] [46] Order
<https://doc.toasttab.com/openapi/orders/tag/Data-definitions/schema/Order/>
[25] Calculating net sales using the orders API - - Developer guide
<https://doc.toasttab.com/doc/devguide/apiOrdersNetSalesCalculation.html>
[28] [29] Authorization via Row Level Security | Supabase Features
<https://supabase.com/features/row-level-security>
[31] Edge Functions | Supabase Docs
<https://supabase.com/docs/guides/functions>
